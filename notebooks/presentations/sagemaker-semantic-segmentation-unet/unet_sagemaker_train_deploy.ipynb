{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demonstrates how to train and deploy a U-net model on Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session(profile_name=\"crayon-site\") # specify your local aws profile\n",
    "sagemaker_session = sagemaker.Session(boto_session)\n",
    "SAGEMAKER_ROLE = \"AmazonSageMaker-ExecutionRole-20200714T182988\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_AUTOMATION_REPO_DIR = \"/home/tailaiw/work/site/ai-automation/\" # specific you local path to the repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = \"resnet50\"\n",
    "pretrained = 1\n",
    "\n",
    "classes = \"asphalt,concrete,rooftop,landscape,gravel\"  # classes to consider, others all merged into \"others\"\n",
    "classes = classes.replace(\" \", \"\")  # make sure no whitespace is included\n",
    "\n",
    "crop_width = 896  # crop from the original image with this size\n",
    "crop_height = 896  # crop from the original image with this size\n",
    "input_width = 224  # model input width (rescale crops to this size before fed to model)\n",
    "input_height = 224  # model input width (rescale crops to this size before fed to model)\n",
    "\n",
    "train_dataset_mode = \"random\"\n",
    "crops_per_train_image = 64  # crops from a training image, if \"random\" mode\n",
    "crops_per_train_image_w = 8  # crops from a training image along horizontal direction, if \"tile\" mode\n",
    "crops_per_train_image_h = 4  # crops from a training image along vertical direction, if \"tile\" mode\n",
    "crops_per_val_image_w = 8  # crops from a validation image along the horizontal direction\n",
    "crops_per_val_image_h = 4  # crops from a validation image along the vertical direction\n",
    "train_ratio = 0.8  # ratio of training data in the entire dataset\n",
    "\n",
    "train_batch_size = 32\n",
    "val_batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "cache_size = 500 # number of images to cache in RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet50-acrlg-224x224-896x896-random-64-8x4-80-32-32\n"
     ]
    }
   ],
   "source": [
    "base_job_name = (\n",
    "    f\"{encoder}-{''.join([cl[0] for cl in classes.split(',')])}-\"\n",
    "    f\"{input_width}x{input_height}-{crop_width}x{crop_height}-\"\n",
    "    f\"{train_dataset_mode}-{crops_per_train_image if train_dataset_mode=='random' else f'{crops_per_train_image_w}x{crops_per_train_image_h}'}-\"\n",
    "    f\"{crops_per_val_image_w}x{crops_per_val_image_h}-\"\n",
    "    f\"{int(train_ratio*100)}-\"\n",
    "    f\"{train_batch_size}-{val_batch_size}\"\n",
    ")\n",
    "print(base_job_name)\n",
    "\n",
    "output_path = f\"s3://st-crayon-dev/tf-outputs/\"\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y-%m-%d-%H-%M-%S-%f\")\n",
    "checkpoint_s3_uri = f\"s3://st-crayon-dev/tf-checkpoints/{base_job_name}_{now}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create estimator and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    entry_point=\"unet.py\",\n",
    "    source_dir=os.path.join(AI_AUTOMATION_REPO_DIR, \"src\", \"unet\"),\n",
    "    role=SAGEMAKER_ROLE,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    use_spot_instances=True,  # uncomment this line to use spot instance\n",
    "    max_run=60 * 60 * 12,  # uncomment this line to use spot instance\n",
    "    max_wait=60 * 60 * 18,  # uncomment this line to use spot instance\n",
    "    framework_version=\"2.1\",\n",
    "    py_version=\"py3\",\n",
    "    base_job_name=base_job_name,\n",
    "    output_path=output_path,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    container_log_level=logging.WARNING,\n",
    "    hyperparameters={\n",
    "        \"encoder\": encoder,\n",
    "        \"pretrained\": pretrained,\n",
    "        \"classes\": classes,\n",
    "        \"input-width\": input_width,\n",
    "        \"input-height\": input_height,\n",
    "        \"crop-width\": crop_width,\n",
    "        \"crop-height\": crop_height,\n",
    "        \"train-dataset-mode\": train_dataset_mode,\n",
    "        \"crops-per-train-image\": crops_per_train_image,\n",
    "        \"crops-per-val-image-w\": crops_per_val_image_w,\n",
    "        \"crops-per-val-image-h\": crops_per_val_image_h,\n",
    "        \"train-batch-size\": train_batch_size,\n",
    "        \"val-batch-size\": val_batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"cache-size\": cache_size,\n",
    "    },\n",
    "    metric_definitions=[\n",
    "        {\n",
    "            \"Name\": \"train:loss\",\n",
    "            \"Regex\": \".*loss: ([0-9\\\\.]+) - mean_io_u[_0-9]*: [0-9\\\\.]+ - categorical_accuracy: [0-9\\\\.]+.*\",\n",
    "        },\n",
    "        {\n",
    "            \"Name\": \"train:categorical_accuracy\",\n",
    "            \"Regex\": \".*loss: [0-9\\\\.]+ - mean_io_u[_0-9]*: [0-9\\\\.]+ - categorical_accuracy: ([0-9\\\\.]+).*\",\n",
    "        },\n",
    "        {\n",
    "            \"Name\": \"train:miou\",\n",
    "            \"Regex\": \".*loss: [0-9\\\\.]+ - mean_io_u[_0-9]*: ([0-9\\\\.]+) - categorical_accuracy: [0-9\\\\.]+.*\",\n",
    "        },\n",
    "        {\n",
    "            \"Name\": \"validation:loss\",\n",
    "            \"Regex\": \".*loss: [0-9\\\\.]+ - mean_io_u[_0-9]*: [0-9\\\\.]+ - categorical_accuracy: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_mean_io_u[_0-9]*: [0-9\\\\.]+ - val_categorical_accuracy: [0-9\\\\.]+.*\",\n",
    "        },\n",
    "        {\n",
    "            \"Name\": \"validation:categorical_accuracy\",\n",
    "            \"Regex\": \".*loss: [0-9\\\\.]+ - mean_io_u[_0-9]*: [0-9\\\\.]+ - categorical_accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_mean_io_u[_0-9]*: [0-9\\\\.]+ - val_categorical_accuracy: ([0-9\\\\.]+).*\",\n",
    "        },\n",
    "        {\n",
    "            \"Name\": \"validation:miou\",\n",
    "            \"Regex\": \".*loss: [0-9\\\\.]+ - mean_io_u[_0-9]*: [0-9\\\\.]+ - categorical_accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_mean_io_u[_0-9]*: ([0-9\\\\.]+) - val_categorical_accuracy: [0-9\\\\.]+.*\",\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_dir = \"s3://st-crayon-dev/annotation/phase_1/export-2020-08-25T18_12_45.454Z\"  # S3 folder where images and masks are saved\n",
    "estimator.fit(input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.p2.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resnet50-acrlg-224x224-896x896-random-6-2020-09-02-21-07-38-621'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete deployed endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "site",
   "language": "python",
   "name": "site"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
