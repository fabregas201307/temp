{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. If you don't specify a bucket, SageMaker SDK will create a default bucket following a pre-defined naming convention in the same region.\n",
    "The IAM role ARN used to give SageMaker access to your data. It can be fetched using the get_execution_role method from sagemaker python SDK if running this notebook in sagemaker studio\n",
    "\n",
    "- profile = aws profile\n",
    "- role = predefined role arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import os\n",
    "import boto3 \n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "role='arn:aws:iam::395166463292:role/service-role/AmazonSageMaker-ExecutionRole-20200714T182988'\n",
    "from PIL import Image\n",
    "\n",
    "profile = 'crayon-site'\n",
    "region_name='us-east-2'\n",
    "bucket = 'st-crayon-dev'\n",
    "prefix = 'sagemaker/labelbox/'\n",
    "\n",
    "session = boto3.session.Session(profile_name = profile, region_name = region_name)\n",
    "sess = sagemaker.Session(session,default_bucket=bucket)\n",
    "print(sess.boto_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Experiment\n",
    "Create an experiment to track all the model training iterations. Experiments are a great way to organize your data science work.  Think of it as a “folder” for organizing your “files”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm = session.client(service_name = 'sagemaker')\n",
    "\n",
    "experiment_name = f'semantic-seg-background'\n",
    "\n",
    "experiments = []\n",
    "\n",
    "for exp in Experiment.list(sagemaker_boto_client=sm):\n",
    "    experiments.append(exp.experiment_name)\n",
    "\n",
    "print(f'List of experiments : {experiments}')\n",
    "\n",
    "if experiment_name not in experiments:\n",
    "    experiment = Experiment.create(experiment_name=experiment_name,\n",
    "                                   description=\"semantic segmentation of drone pictures\",\n",
    "                                   sagemaker_boto_client=sm)\n",
    "\n",
    "print(f'Experiment used for notebook = {experiment_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Experiment\n",
    "Create a Trial for each training run to track it's inputs, parameters, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_samples = 2397 # 2397 and 11294\n",
    "backbone = 'resnet-50'  # resnet-101\n",
    "base_size = 512\n",
    "crop_size = 384\n",
    "num_epochs = 90\n",
    "algorithm = 'fcn'  # 'psp' 'deeplab'\n",
    "tiles_type = 'tiles_1024'\n",
    "learning_rate = 0.0001\n",
    "optimizer='adam' # 'sgd' 'adam', 'rmsprop', 'nag', 'adagrad'.\n",
    "lrs='cosine' # 'poly' 'cosine' and 'step'.                           \n",
    "rand_string = str(np.random.randint(100)).zfill(2)\n",
    "base_trial_name = f\"I{tiles_type.replace('_','-')}-B{base_size}-C{crop_size}-E{num_epochs}-{algorithm}-LR{str(learning_rate).split('.')[1]}-O{optimizer}-LRS{lrs}-{rand_string}\"\n",
    "# Add some randomness to each trial name\n",
    "trial_name = f'ssl-{base_trial_name}'\n",
    "ss_trial = Trial.create(trial_name = trial_name,\n",
    "                          experiment_name = experiment_name,\n",
    "                          sagemaker_boto_client = sm,\n",
    "                          tags = [{'Key': 'experiment_name', 'Value': 'aws-ss-drone-dataset'}])\n",
    "ss_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_trial_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = f's3://{bucket}/{prefix}output'\n",
    "s3_output_checkpoints = f's3://{bucket}/{prefix}output/checkpoints/{base_trial_name}'\n",
    "s3_output_location, s3_output_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training image\n",
    "Since we are using prebaked aws semantic segmentation algo, we need the Amazon SageMaker Semantic Segmentaion docker image, which is static and need not be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipe Mode\n",
    "\n",
    "In File-mode training data is downloaded to an encrypted EBS volume prior to commencing training. Once downloaded, the training algorithm simply trains by reading the downloaded training data files.\n",
    "\n",
    "On the other hand, in Pipe-mode the input data is transferred to the algorithm while it is training. This poses a few significant advantages over File-mode:\n",
    "\n",
    "\n",
    "*  In File-mode, training startup time is proportional to size of the input data. In Pipe-mode, the startup delay is constant, independent of the size of the input data. This translates to much faster training startup for training jobs with large GB/PB-scale training datasets.\n",
    "* You do not need to allocate (and pay for) a large disk volume to be able to download the dataset.\n",
    "* Throughput on IO-bound Pipe-mode algorithms can be multiple times faster than on equivalent File-mode algorithms.\n",
    "\n",
    "However, these advantages come at a cost - a more complicated programming model than simply reading from files on a disk. This notebook aims to clarify what you need to do in order to use Pipe-mode in your custom training algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare handshake \n",
    "between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. In pipe mode data channels are the manifest files which contain `s3` location of images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "manifest_train = f'data/raw/imgs/{tiles_type}/manifests/manifest_file_train_imgs.json'\n",
    "manifest_train = f's3://{bucket}/{manifest_train}'\n",
    "manifest_val = f'data/raw/imgs/{tiles_type}/manifests/manifest_file_val_imgs.json'\n",
    "manifest_val = f's3://{bucket}/{manifest_val}'\n",
    "\n",
    "print(f'{manifest_train=}\\n{manifest_val=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distribution = 'FullyReplicated'\n",
    "# Create sagemaker s3_input objects\n",
    "train_data = sagemaker.session.s3_input(manifest_train, \n",
    "                                        distribution=distribution, \n",
    "                                        content_type='application/x-recordio',\n",
    "                                        s3_data_type='AugmentedManifestFile',\n",
    "                                        attribute_names=['source-ref', 'annotation-ref'],\n",
    "                                        input_mode='Pipe',\n",
    "                                        record_wrapping=\"RecordIO\")\n",
    "validation_data = sagemaker.session.s3_input(manifest_val, \n",
    "                                        distribution=distribution, \n",
    "                                        content_type='application/x-recordio',\n",
    "                                        s3_data_type='AugmentedManifestFile',\n",
    "                                        attribute_names=['source-ref', 'annotation-ref'],\n",
    "                                        input_mode='Pipe',\n",
    "                                        record_wrapping=\"RecordIO\")\n",
    "\n",
    "# s3model = 's3://st-crayon-dev/sagemaker/labelbox/output/ssl-job-Itiles-1024-B256-C224-E80-fcn-N-2020-08-17-19-55-46-533/output/model.tar.gz'\n",
    "# model_data = sagemaker.session.s3_input(s3model, distribution= 'FullyReplicated',s3_data_type='S3Prefix', input_mode='File', content_type='application/x-sagemaker-model')\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data}\n",
    "# data_channels = {'train': train_data, 'validation': validation_data, 'model': model_data}\n",
    "data_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "To begin training, we have to create ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job. we name our training job as ``ss-labelbox-train``. For training we need a gpu insatance type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_job_name = f'ssl-job-{base_trial_name}'\n",
    "\n",
    "ss_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role=role, \n",
    "                                         train_instance_count = 1, \n",
    "                                         train_instance_type = 'ml.p3.2xlarge',\n",
    "                                         train_volume_size = 20,\n",
    "                                         train_max_run = 60 * 60 * 12,\n",
    "                                         train_max_wait= 60 * 60 * 18,\n",
    "                                         output_path = s3_output_location,\n",
    "                                         checkpoint_s3_uri = s3_output_checkpoints,\n",
    "                                         base_job_name = base_job_name,\n",
    "                                         train_use_spot_instances=True,\n",
    "                                         input_mode='Pipe',\n",
    "                                         sagemaker_session = sess,\n",
    "#                                          model_uri=ss_model,\n",
    "                                         enable_sagemaker_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantic segmentation algorithm at its core has two compoenents.\n",
    "\n",
    "- An encoder or backbone network,\n",
    "- A decoder or algorithm network. \n",
    "\n",
    "The encoder or backbone network is typically a regular convolutional neural network that may or maynot have had their layers pre-trained on an alternate task such as the [classification task of ImageNet images](http://www.image-net.org/). The Amazon SageMaker Semantic Segmentation algorithm comes with two choices of pre-trained or to be trained-from-scratch backbone networks ([ResNets](https://arxiv.org/abs/1512.03385) 50 or 101). \n",
    "\n",
    "The decoder is a network that picks up the outputs of one or many layers from the backbone and reconstructs the segmentation mask from it. Amazon SageMaker Semantic Segmentation algorithm comes with a choice of the [Fully-convolutional network (FCN)](https://arxiv.org/abs/1605.06211), the [Pyramid scene parsing (PSP) network](https://arxiv.org/abs/1612.01105) and [deeplab v3] (https://arxiv.org/abs/1706.05587)\n",
    "\n",
    "The algorithm also has ample options for hyperparameters that help configure the training job. The next step in our training, is to setup these networks and hyperparameters. Consider the following example definition of hyperparameters. See the SageMaker Semantic Segmentation [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html) for more details on the hyperparameters.\n",
    "\n",
    "One of the hyperparameters here for instance is the `epochs`. This defines how many passes of the dataset we iterate over and determines that training time of the algorithm. Based on our tests, train the model for `x` epochs with similar settings should give us 'reasonable' segmentation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_model.set_hyperparameters(backbone=backbone, # This is the encoder. Other option is resnet-50\n",
    "                             algorithm=algorithm, # This is the decoder. Other option is 'fcn', 'psp' and 'deeplab'                             \n",
    "                             use_pretrained_model=True, # Use the pre-trained model.\n",
    "                             base_size=base_size,\n",
    "                             crop_size=crop_size, # Size of image random crop.                              \n",
    "                             num_classes=6, # Pascal has 21 classes. This is a mandatory parameter.\n",
    "                             epochs=num_epochs, # Number of epochs to run.\n",
    "                             learning_rate=learning_rate,                             \n",
    "                             optimizer=optimizer, # Other options include 'adam', 'rmsprop', 'nag', 'adagrad'.\n",
    "                             lr_scheduler=lrs, # Other options include 'cosine' and 'step'.                           \n",
    "                             mini_batch_size=8, # Setup some mini batch size.\n",
    "                             validation_mini_batch_size=8,\n",
    "                             early_stopping=True, # Turn on early stopping. If OFF, other early stopping parameters are ignored.\n",
    "                             early_stopping_patience=6, # Tolerate these many epochs if the mIoU doens't increase.\n",
    "                             early_stopping_tolerance=.001,\n",
    "                             early_stopping_min_epochs=12, # No matter what, run these many number of epochs.                             \n",
    "                             num_training_samples=num_training_samples) # This is a mandatory parameter, 1464 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'{train_data.config}\\n{validation_data.config}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ss_model.fit(inputs=data_channels,\n",
    "             logs=True,\n",
    "             experiment_config={\n",
    "                \"TrialName\": ss_trial.trial_name,\n",
    "                \"TrialComponentDisplayName\": \"Training\",\n",
    "                },\n",
    "             wait=True\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
