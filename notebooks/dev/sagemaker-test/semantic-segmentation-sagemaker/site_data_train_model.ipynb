{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596786403905",
   "display_name": "Python 3.8.0 64-bit ('tensorflow': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. If you don't specify a bucket, SageMaker SDK will create a default bucket following a pre-defined naming convention in the same region.\n",
    "The IAM role ARN used to give SageMaker access to your data. It can be fetched using the get_execution_role method from sagemaker python SDK if running this notebook in sagemaker studio\n",
    "\n",
    "- profile = aws profile\n",
    "- role = predefined role arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\nSession(region_name='us-east-2')\nCPU times: user 469 ms, sys: 578 ms, total: 1.05 s\nWall time: 3.31 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import os\n",
    "import boto3 \n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "role='arn:aws:iam::395166463292:role/service-role/AmazonSageMaker-ExecutionRole-20200714T182988'\n",
    "from PIL import Image\n",
    "\n",
    "profile = 'sites'\n",
    "region_name='us-east-2'\n",
    "bucket = 'st-crayon-dev'\n",
    "prefix = 'sagemaker/labelbox/'\n",
    "\n",
    "session = boto3.session.Session(profile_name = profile, region_name = region_name)\n",
    "sess = sagemaker.Session(session,default_bucket=bucket)\n",
    "print(sess.boto_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Experiment\n",
    "Create an experiment to track all the model training iterations. Experiments are a great way to organize your data science work.  Think of it as a “folder” for organizing your “files”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "List of experiments : ['labelbox-semantic-segmentation512', 'site-tech-drone-img-seg']\nExperiment used for notebook = site-tech-drone-img-seg\n"
    }
   ],
   "source": [
    "sm = session.client(service_name = 'sagemaker')\n",
    "\n",
    "experiment_name = f'site-tech-drone-img-seg'\n",
    "\n",
    "experiments = []\n",
    "\n",
    "for exp in Experiment.list(sagemaker_boto_client=sm):\n",
    "    experiments.append(exp.experiment_name)\n",
    "\n",
    "print(f'List of experiments : {experiments}')\n",
    "\n",
    "if experiment_name not in experiments:\n",
    "    experiment = Experiment.create(experiment_name=experiment_name,\n",
    "                                   description=\"semantic segmentation of drone pictures\",\n",
    "                                   sagemaker_boto_client=sm)\n",
    "experiment_name = experiments[1]                                   \n",
    "print(f'Experiment used for notebook = {experiment_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Experiment\n",
    "Create a Trial for each training run to track it's inputs, parameters, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Trial(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7f22af756a90>,trial_name='semantic-segmentation-labelbox-dataset-1024-1596790424',experiment_name='site-tech-drone-img-seg',tags=[{'Key': 'experiment_name', 'Value': 'aws-ss-drone-dataset'}],trial_arn='arn:aws:sagemaker:us-east-2:395166463292:experiment-trial/semantic-segmentation-labelbox-dataset-1024-1596790424',response_metadata={'RequestId': 'cab39a80-bbef-427f-99b8-ff6b6b62b296', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'cab39a80-bbef-427f-99b8-ff6b6b62b296', 'content-type': 'application/x-amz-json-1.1', 'content-length': '127', 'date': 'Fri, 07 Aug 2020 08:53:44 GMT'}, 'RetryAttempts': 0})"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "trial_name = f'semantic-segmentation-labelbox-dataset-1024-{int(time.time())}'\n",
    "ss_trial = Trial.create(trial_name = trial_name,\n",
    "                          experiment_name = experiment_name,\n",
    "                          sagemaker_boto_client = sm,\n",
    "                          tags = [{'Key': 'experiment_name', 'Value': 'aws-ss-drone-dataset'}])\n",
    "ss_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'s3://st-crayon-dev/sagemaker/labelbox/output'"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "s3_output_location = f's3://{bucket}/{prefix}output'\n",
    "s3_output_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training image\n",
    "Since we are using prebaked aws semantic segmentation algo, we need the Amazon SageMaker Semantic Segmentaion docker image, which is static and need not be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:sagemaker.amazon.amazon_estimator:'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n825641698319.dkr.ecr.us-east-2.amazonaws.com/semantic-segmentation:latest\n"
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training File Mode (We plan to depricate using file mode. \n",
    "## Therefore move to section Training Pipe mode)\n",
    "To begin training, we have to create ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job. we name our training job as ``ss-labelbox-train``. For training we need a gpu insatance type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
    }
   ],
   "source": [
    "base_job_name = f'ss-labelbox-train-{int(time.time())}'\n",
    "\n",
    "ss_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role=role, \n",
    "                                         train_instance_count = 1, \n",
    "                                         train_instance_type = 'ml.p2.8xlarge',\n",
    "                                         train_volume_size = 10,\n",
    "                                         train_max_run = 7200,\n",
    "                                         train_max_wait=7200,\n",
    "                                         output_path = s3_output_location,\n",
    "                                         base_job_name = base_job_name,\n",
    "                                         train_use_spot_instances=True,\n",
    "                                         input_mode='File',\n",
    "                                         sagemaker_session = sess,\n",
    "                                         enable_sagemaker_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_model.set_hyperparameters(backbone='resnet-50', # This is the encoder. Other option is resnet-50\n",
    "                             algorithm='deeplab', # This is the decoder. Other option is 'psp' and 'deeplab'                             \n",
    "                             use_pretrained_model='True', # Use the pre-trained model.\n",
    "                             crop_size=512, # Size of image random crop.     \n",
    "                             base_size=1024,                        \n",
    "                             num_classes=8, # Pascal has 21 classes. This is a mandatory parameter.\n",
    "                             epochs=20, # Number of epochs to run.\n",
    "                             learning_rate=0.0001,                             \n",
    "                             optimizer='rmsprop', # Other options include 'adam', 'rmsprop', 'nag', 'adagrad'.\n",
    "                             lr_scheduler='poly', # Other options include 'cosine' and 'step'.                           \n",
    "                             mini_batch_size=32, # Setup some mini batch size.\n",
    "                             validation_mini_batch_size=32,\n",
    "                             early_stopping=True, # Turn on early stopping. If OFF, other early stopping parameters are ignored.\n",
    "                             early_stopping_patience=5, # Tolerate these many epochs if the mIoU doens't increase.\n",
    "                             early_stopping_tolerance=.001,\n",
    "                             early_stopping_min_epochs=2, # No matter what, run these many number of epochs.                             \n",
    "                             num_training_samples=1190) # This is a mandatory parameter, 1464 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = prefix + 'first-batch/train_tiles1024'\n",
    "validation_channel = prefix + 'first-batch/valid_tiles1024'\n",
    "train_annotation_channel = prefix + 'first-batch/train_annotation_tiles1024'\n",
    "validation_annotation_channel = prefix + 'first-batch/valid_annotation_tiles1024'\n",
    "\n",
    "s3_train_data = f's3://{bucket}/{train_channel}/'\n",
    "s3_validation_data = f's3://{bucket}/{validation_channel}/'\n",
    "s3_train_annotation = f's3://{bucket}/{train_annotation_channel}/'\n",
    "s3_validation_annotation = f's3://{bucket}/{validation_annotation_channel}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "s3_train_data='s3://st-crayon-dev/sagemaker/labelbox/first-batch/train_tiles1024/'\ns3_train_annotation='s3://st-crayon-dev/sagemaker/labelbox/first-batch/train_annotation_tiles1024/'\ns3_validation_data='s3://st-crayon-dev/sagemaker/labelbox/first-batch/valid_tiles1024/'\ns3_validation_annotation='s3://st-crayon-dev/sagemaker/labelbox/first-batch/valid_annotation_tiles1024/'\n"
    }
   ],
   "source": [
    "print(f'{s3_train_data=}\\n{s3_train_annotation=}\\n{s3_validation_data=}\\n{s3_validation_annotation=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\nWARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\nWARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\nWARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'train': <sagemaker.inputs.s3_input at 0x7f1af8ef57f0>,\n 'validation': <sagemaker.inputs.s3_input at 0x7f1af8ef58e0>,\n 'train_annotation': <sagemaker.inputs.s3_input at 0x7f1af8ef5580>,\n 'validation_annotation': <sagemaker.inputs.s3_input at 0x7f1af9c5cc40>}"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "distribution = 'FullyReplicated'\n",
    "# Create sagemaker s3_input objects\n",
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution=distribution, \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution=distribution, \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "train_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "validation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data,\n",
    "                 'train_annotation': train_annotation, \n",
    "                 'validation_annotation':validation_annotation}\n",
    "data_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "put_metric. host: algo-1, epoch: 9, validation throughput: 57.9324090311 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:10:54 INFO 140367851128640] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n\u001b[34m[08/02/2020 20:10:54 INFO 140367851128640] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1596399054.551507, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 9}, \"StartTime\": 1596398984.574421}\n\u001b[0m\n\u001b[34m[08/02/2020 20:11:31 INFO 140367851128640] #progress_notice. epoch: 10, iterations: 20 speed: 21.8046023078 samples/sec learning_rate: 0.000051\u001b[0m\n\u001b[34m[08/02/2020 20:11:58 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 10, train loss: 0.4847308942361882 .\u001b[0m\n\u001b[34m[08/02/2020 20:11:58 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 10, train throughput: 21.0094409178 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:12:05 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 10, validation pixel_accuracy: 0.8859664658513535 .\u001b[0m\n\u001b[34m[08/02/2020 20:12:05 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 10, validation mIOU: 0.4858692575569343 .\u001b[0m\n\u001b[34m[08/02/2020 20:12:05 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 10, validation throughput: 58.119439833 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:12:05 INFO 140367851128640] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n\u001b[34m[08/02/2020 20:12:05 INFO 140367851128640] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}}, \"EndTime\": 1596399125.697936, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 10}, \"StartTime\": 1596399054.551665}\n\u001b[0m\n\u001b[34m[08/02/2020 20:12:42 INFO 140367851128640] #progress_notice. epoch: 11, iterations: 20 speed: 21.626077153 samples/sec learning_rate: 0.000046\u001b[0m\n\u001b[34m[08/02/2020 20:13:07 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 11, train loss: 0.4400611866164852 .\u001b[0m\n\u001b[34m[08/02/2020 20:13:07 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 11, train throughput: 21.0140079636 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:13:15 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 11, validation pixel_accuracy: 0.8840941566091861 .\u001b[0m\n\u001b[34m[08/02/2020 20:13:15 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 11, validation mIOU: 0.4826293152625237 .\u001b[0m\n\u001b[34m[08/02/2020 20:13:15 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 11, validation throughput: 58.364057321 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:13:15 INFO 140367851128640] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1596399195.060329, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 11}, \"StartTime\": 1596399125.698124}\n\u001b[0m\n\u001b[34m[08/02/2020 20:13:52 INFO 140367851128640] #progress_notice. epoch: 12, iterations: 20 speed: 21.8661816556 samples/sec learning_rate: 0.000041\u001b[0m\n\u001b[34m[08/02/2020 20:14:17 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 12, train loss: 0.4062378410551999 .\u001b[0m\n\u001b[34m[08/02/2020 20:14:17 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 12, train throughput: 20.8450930144 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:14:24 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 12, validation pixel_accuracy: 0.8833012279511926 .\u001b[0m\n\u001b[34m[08/02/2020 20:14:24 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 12, validation mIOU: 0.486201520411694 .\u001b[0m\n\u001b[34m[08/02/2020 20:14:24 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 12, validation throughput: 58.5351573145 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:14:24 INFO 140367851128640] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n\u001b[34m[08/02/2020 20:14:24 INFO 140367851128640] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 13, \"sum\": 13.0, \"min\": 13}}, \"EndTime\": 1596399264.391136, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 12}, \"StartTime\": 1596399195.060464}\n\u001b[0m\n\u001b[34m[08/02/2020 20:15:01 INFO 140367851128640] #progress_notice. epoch: 13, iterations: 20 speed: 21.6681790414 samples/sec learning_rate: 0.000036\u001b[0m\n\u001b[34m[08/02/2020 20:15:27 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 13, train loss: 0.4009730312469843 .\u001b[0m\n\u001b[34m[08/02/2020 20:15:27 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 13, train throughput: 20.8957095187 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:15:34 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 13, validation pixel_accuracy: 0.8903509038022064 .\u001b[0m\n\u001b[34m[08/02/2020 20:15:34 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 13, validation mIOU: 0.48821373177005073 .\u001b[0m\n\u001b[34m[08/02/2020 20:15:34 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 13, validation throughput: 58.6328019186 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:15:34 INFO 140367851128640] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n\u001b[34m[08/02/2020 20:15:34 INFO 140367851128640] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1596399334.527897, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 13}, \"StartTime\": 1596399264.391272}\n\u001b[0m\n\u001b[34m[08/02/2020 20:16:12 INFO 140367851128640] #progress_notice. epoch: 14, iterations: 20 speed: 21.8326438198 samples/sec learning_rate: 0.000031\u001b[0m\n\u001b[34m[08/02/2020 20:16:37 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 14, train loss: 0.3671483562604801 .\u001b[0m\n\u001b[34m[08/02/2020 20:16:37 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 14, train throughput: 21.0772902573 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:16:44 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 14, validation pixel_accuracy: 0.895224096158967 .\u001b[0m\n\u001b[34m[08/02/2020 20:16:44 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 14, validation mIOU: 0.49363064078829577 .\u001b[0m\n\u001b[34m[08/02/2020 20:16:44 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 14, validation throughput: 58.5413078246 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:16:44 INFO 140367851128640] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n\u001b[34m[08/02/2020 20:16:44 INFO 140367851128640] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 15, \"sum\": 15.0, \"min\": 15}}, \"EndTime\": 1596399404.862096, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 14}, \"StartTime\": 1596399334.528034}\n\u001b[0m\n\u001b[34m[08/02/2020 20:17:22 INFO 140367851128640] #progress_notice. epoch: 15, iterations: 20 speed: 21.6980030451 samples/sec learning_rate: 0.000026\u001b[0m\n\u001b[34m[08/02/2020 20:17:49 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 15, train loss: 0.3789590150117874 .\u001b[0m\n\u001b[34m[08/02/2020 20:17:49 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 15, train throughput: 20.9166479435 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:17:56 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 15, validation pixel_accuracy: 0.8958257058414649 .\u001b[0m\n\u001b[34m[08/02/2020 20:17:56 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 15, validation mIOU: 0.4948797968461853 .\u001b[0m\n\u001b[34m[08/02/2020 20:17:56 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 15, validation throughput: 58.391451708 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:17:56 INFO 140367851128640] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n\u001b[34m[08/02/2020 20:17:56 INFO 140367851128640] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1596399476.380877, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 15}, \"StartTime\": 1596399404.86223}\n\u001b[0m\n\u001b[34m[08/02/2020 20:18:33 INFO 140367851128640] #progress_notice. epoch: 16, iterations: 20 speed: 21.9745050823 samples/sec learning_rate: 0.000020\u001b[0m\n\u001b[34m[08/02/2020 20:18:58 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 16, train loss: 0.36558090754457423 .\u001b[0m\n\u001b[34m[08/02/2020 20:18:58 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 16, train throughput: 21.0576836307 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:19:05 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 16, validation pixel_accuracy: 0.8882373709285613 .\u001b[0m\n\u001b[34m[08/02/2020 20:19:05 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 16, validation mIOU: 0.48986225981945136 .\u001b[0m\n\u001b[34m[08/02/2020 20:19:05 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 16, validation throughput: 58.3971355246 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:19:05 INFO 140367851128640] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 17, \"sum\": 17.0, \"min\": 17}}, \"EndTime\": 1596399545.236561, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 16}, \"StartTime\": 1596399476.381057}\n\u001b[0m\n\u001b[34m[08/02/2020 20:19:42 INFO 140367851128640] #progress_notice. epoch: 17, iterations: 20 speed: 22.0100252098 samples/sec learning_rate: 0.000015\u001b[0m\n\u001b[34m[08/02/2020 20:20:07 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 17, train loss: 0.3645332624783387 .\u001b[0m\n\u001b[34m[08/02/2020 20:20:07 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 17, train throughput: 20.8847647051 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:20:14 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 17, validation pixel_accuracy: 0.8956523985501276 .\u001b[0m\n\u001b[34m[08/02/2020 20:20:14 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 17, validation mIOU: 0.4946498255134544 .\u001b[0m\n\u001b[34m[08/02/2020 20:20:14 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 17, validation throughput: 58.3745860243 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:20:14 INFO 140367851128640] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1596399614.934657, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 17}, \"StartTime\": 1596399545.236707}\n\u001b[0m\n\u001b[34m[08/02/2020 20:20:52 INFO 140367851128640] #progress_notice. epoch: 18, iterations: 20 speed: 21.6746069978 samples/sec learning_rate: 0.000009\u001b[0m\n\u001b[34m[08/02/2020 20:21:17 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 18, train loss: 0.37526731112518824 .\u001b[0m\n\u001b[34m[08/02/2020 20:21:17 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 18, train throughput: 21.003017697 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:21:24 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 18, validation pixel_accuracy: 0.8964647635319198 .\u001b[0m\n\u001b[34m[08/02/2020 20:21:24 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 18, validation mIOU: 0.49613986421056816 .\u001b[0m\n\u001b[34m[08/02/2020 20:21:24 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 18, validation throughput: 58.5375716213 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:21:24 INFO 140367851128640] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n\u001b[34m[08/02/2020 20:21:25 INFO 140367851128640] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}}, \"EndTime\": 1596399685.165936, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 18}, \"StartTime\": 1596399614.934796}\n\u001b[0m\n\u001b[34m[08/02/2020 20:22:02 INFO 140367851128640] #progress_notice. epoch: 19, iterations: 20 speed: 21.790573169 samples/sec learning_rate: 0.000003\u001b[0m\n\u001b[34m[08/02/2020 20:22:27 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 19, train loss: 0.3362759690026979 .\u001b[0m\n\u001b[34m[08/02/2020 20:22:27 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 19, train throughput: 21.0924593341 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:22:34 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 19, validation pixel_accuracy: 0.8971040128844755 .\u001b[0m\n\u001b[34m[08/02/2020 20:22:34 INFO 140367851128640] #quality_metric. host: algo-1, epoch: 19, validation mIOU: 0.4966998897581505 .\u001b[0m\n\u001b[34m[08/02/2020 20:22:34 INFO 140367851128640] #throughput_metric. host: algo-1, epoch: 19, validation throughput: 57.9871335299 samples/sec.\u001b[0m\n\u001b[34m[08/02/2020 20:22:34 INFO 140367851128640] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n\u001b[34m[08/02/2020 20:22:34 INFO 140367851128640] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1596399754.796308, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 19}, \"StartTime\": 1596399685.166087}\n\u001b[0m\n\u001b[34m[08/02/2020 20:22:34 WARNING 140367851128640] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n\u001b[34m[08/02/2020 20:22:34 INFO 140367851128640] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n\u001b[34m[08/02/2020 20:22:34 INFO 140367851128640] Test data is not provided.\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}, \"totaltime\": {\"count\": 1, \"max\": 1408090.656042099, \"sum\": 1408090.656042099, \"min\": 1408090.656042099}, \"setuptime\": {\"count\": 1, \"max\": 18.699169158935547, \"sum\": 18.699169158935547, \"min\": 18.699169158935547}}, \"EndTime\": 1596399755.102184, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\"}, \"StartTime\": 1596398348.485925}\n\u001b[0m\n\n2020-08-02 20:22:36 Uploading - Uploading generated training model\n2020-08-02 20:23:23 Completed - Training job completed\nTraining seconds: 1519\nBillable seconds: 456\nManaged Spot Training savings: 70.0%\n"
    }
   ],
   "source": [
    "ss_model.fit(inputs=data_channels,\n",
    "             logs=True,\n",
    "             experiment_config={\n",
    "                \"TrialName\": ss_trial.trial_name,\n",
    "                \"TrialComponentDisplayName\": \"Training\",\n",
    "                },\n",
    "             wait=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipe Mode\n",
    "\n",
    "In File-mode training data is downloaded to an encrypted EBS volume prior to commencing training. Once downloaded, the training algorithm simply trains by reading the downloaded training data files.\n",
    "\n",
    "On the other hand, in Pipe-mode the input data is transferred to the algorithm while it is training. This poses a few significant advantages over File-mode:\n",
    "\n",
    "\n",
    "*  In File-mode, training startup time is proportional to size of the input data. In Pipe-mode, the startup delay is constant, independent of the size of the input data. This translates to much faster training startup for training jobs with large GB/PB-scale training datasets.\n",
    "* You do not need to allocate (and pay for) a large disk volume to be able to download the dataset.\n",
    "* Throughput on IO-bound Pipe-mode algorithms can be multiple times faster than on equivalent File-mode algorithms.\n",
    "\n",
    "However, these advantages come at a cost - a more complicated programming model than simply reading from files on a disk. This notebook aims to clarify what you need to do in order to use Pipe-mode in your custom training algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare manifest file for data\n",
    "Lets look at the python script which create the manifest files for training in pipe mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../../../../src/site_tools/site_tools/site_data_manifestfiles.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\nINFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\ntrain_imgs 2361\ndata/raw/imgs/tiles_1024/manifests/manifest_file_train_imgs.json\n{'ResponseMetadata': {'RequestId': 'DQ9S6PCZ5W0QENEW', 'HostId': 'df3JuU8Vw4BVnTafs4sTM4Bccy6PSZCgIVyiWbMfXp3ZvXT/OKdUHvbo618RbkTFND9tvfhPGjs=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'df3JuU8Vw4BVnTafs4sTM4Bccy6PSZCgIVyiWbMfXp3ZvXT/OKdUHvbo618RbkTFND9tvfhPGjs=', 'x-amz-request-id': 'DQ9S6PCZ5W0QENEW', 'date': 'Fri, 07 Aug 2020 10:43:57 GMT', 'etag': '\"eedd93b4e6c878e557452d6ed700815c\"', 'content-length': '0', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'ETag': '\"eedd93b4e6c878e557452d6ed700815c\"'}\ntest_imgs 292\ndata/raw/imgs/tiles_1024/manifests/manifest_file_test_imgs.json\n{'ResponseMetadata': {'RequestId': 'DFF37CA47DFF23C2', 'HostId': 'Ec+UAoEYsHmIughfufPtLLLRpE156TVMXCqQTlGcarkQSWyODbWXn3Z3qtVi2z7MSV1wHEWhfnQ=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'Ec+UAoEYsHmIughfufPtLLLRpE156TVMXCqQTlGcarkQSWyODbWXn3Z3qtVi2z7MSV1wHEWhfnQ=', 'x-amz-request-id': 'DFF37CA47DFF23C2', 'date': 'Fri, 07 Aug 2020 10:43:58 GMT', 'etag': '\"9f0374518c16e7a56c117c1957f0f642\"', 'content-length': '0', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'ETag': '\"9f0374518c16e7a56c117c1957f0f642\"'}\nval_imgs 263\ndata/raw/imgs/tiles_1024/manifests/manifest_file_val_imgs.json\n{'ResponseMetadata': {'RequestId': '7FE1F33A442C3FE0', 'HostId': 'qyVx+tiXV5CyNtBi0GLiw9sGFM49UdUgF23yOyNOn2OXkUP9nVIg/J8U9D52FdFiVLihcxcD514=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'qyVx+tiXV5CyNtBi0GLiw9sGFM49UdUgF23yOyNOn2OXkUP9nVIg/J8U9D52FdFiVLihcxcD514=', 'x-amz-request-id': '7FE1F33A442C3FE0', 'date': 'Fri, 07 Aug 2020 10:43:59 GMT', 'etag': '\"b3375a339ef7163ad3a8187bbbf0e94f\"', 'content-length': '0', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'ETag': '\"b3375a339ef7163ad3a8187bbbf0e94f\"'}\n"
    }
   ],
   "source": [
    "%run ../../../../src/site_tools/site_tools/site_data_manifestfiles.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare handshake \n",
    "between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. In pipe mode data channels are the manifest files which contain `s3` location of images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "manifest_train='s3://st-crayon-dev/data/raw/imgs/tiles_1024/manifests/manifest_file_train_imgs.json'\nmanifest_val='s3://st-crayon-dev/data/raw/imgs/tiles_1024/manifests/manifest_file_val_imgs.json'\n"
    }
   ],
   "source": [
    "manifest_train = 'data/raw/imgs/tiles_1024/manifests/manifest_file_train_imgs.json'\n",
    "manifest_train = f's3://{bucket}/{manifest_train}'\n",
    "manifest_val = 'data/raw/imgs/tiles_1024/manifests/manifest_file_val_imgs.json'\n",
    "manifest_val = f's3://{bucket}/{manifest_val}'\n",
    "\n",
    "print(f'{manifest_train=}\\n{manifest_val=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\nWARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'train': <sagemaker.inputs.s3_input at 0x7f22ae64a400>,\n 'validation': <sagemaker.inputs.s3_input at 0x7f22af7f1430>}"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "distribution = 'FullyReplicated'\n",
    "# Create sagemaker s3_input objects\n",
    "train_data = sagemaker.session.s3_input(manifest_train, \n",
    "                                        distribution=distribution, \n",
    "                                        content_type='application/x-recordio',\n",
    "                                        s3_data_type='AugmentedManifestFile',\n",
    "                                        attribute_names=['source-ref', 'annotation-ref'],\n",
    "                                        input_mode='Pipe',\n",
    "                                        record_wrapping=\"RecordIO\")\n",
    "validation_data = sagemaker.session.s3_input(manifest_val, \n",
    "                                        distribution=distribution, \n",
    "                                        content_type='application/x-recordio',\n",
    "                                        s3_data_type='AugmentedManifestFile',\n",
    "                                        attribute_names=['source-ref', 'annotation-ref'],\n",
    "                                        input_mode='Pipe',\n",
    "                                        record_wrapping=\"RecordIO\")\n",
    "\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data}\n",
    "data_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "To begin training, we have to create ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job. we name our training job as ``ss-labelbox-train``. For training we need a gpu insatance type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
    }
   ],
   "source": [
    "base_job_name = f'ss-labelbox-train-pipe-{int(time.time())}'\n",
    "\n",
    "ss_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role=role, \n",
    "                                         train_instance_count = 1, \n",
    "                                         train_instance_type = 'ml.p2.8xlarge',\n",
    "                                         train_volume_size = 10,\n",
    "                                         train_max_run = 7200,\n",
    "                                         train_max_wait=7200,\n",
    "                                         output_path = s3_output_location,\n",
    "                                         base_job_name = base_job_name,\n",
    "                                         train_use_spot_instances=True,\n",
    "                                         input_mode='Pipe',\n",
    "                                         sagemaker_session = sess,\n",
    "                                         enable_sagemaker_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantic segmentation algorithm at its core has two compoenents.\n",
    "\n",
    "- An encoder or backbone network,\n",
    "- A decoder or algorithm network. \n",
    "\n",
    "The encoder or backbone network is typically a regular convolutional neural network that may or maynot have had their layers pre-trained on an alternate task such as the [classification task of ImageNet images](http://www.image-net.org/). The Amazon SageMaker Semantic Segmentation algorithm comes with two choices of pre-trained or to be trained-from-scratch backbone networks ([ResNets](https://arxiv.org/abs/1512.03385) 50 or 101). \n",
    "\n",
    "The decoder is a network that picks up the outputs of one or many layers from the backbone and reconstructs the segmentation mask from it. Amazon SageMaker Semantic Segmentation algorithm comes with a choice of the [Fully-convolutional network (FCN)](https://arxiv.org/abs/1605.06211), the [Pyramid scene parsing (PSP) network](https://arxiv.org/abs/1612.01105) and [deeplab v3] (https://arxiv.org/abs/1706.05587)\n",
    "\n",
    "The algorithm also has ample options for hyperparameters that help configure the training job. The next step in our training, is to setup these networks and hyperparameters. Consider the following example definition of hyperparameters. See the SageMaker Semantic Segmentation [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html) for more details on the hyperparameters.\n",
    "\n",
    "One of the hyperparameters here for instance is the `epochs`. This defines how many passes of the dataset we iterate over and determines that training time of the algorithm. Based on our tests, train the model for `x` epochs with similar settings should give us 'reasonable' segmentation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_model.set_hyperparameters(backbone='resnet-50', # This is the encoder. Other option is resnet-50\n",
    "                             algorithm='deeplab', # This is the decoder. Other option is 'fcn', 'psp' and 'deeplab'                             \n",
    "                             use_pretrained_model='True', # Use the pre-trained model.\n",
    "                             base_size=1024,\n",
    "                             crop_size=448, # Size of image random crop.                              \n",
    "                             num_classes=8, # Pascal has 21 classes. This is a mandatory parameter.\n",
    "                             epochs=20, # Number of epochs to run.\n",
    "                             learning_rate=0.0001,                             \n",
    "                             optimizer='rmsprop', # Other options include 'adam', 'rmsprop', 'nag', 'adagrad'.\n",
    "                             lr_scheduler='poly', # Other options include 'cosine' and 'step'.                           \n",
    "                             mini_batch_size=16, # Setup some mini batch size.\n",
    "                             validation_mini_batch_size=16,\n",
    "                             early_stopping=True, # Turn on early stopping. If OFF, other early stopping parameters are ignored.\n",
    "                             early_stopping_patience=2, # Tolerate these many epochs if the mIoU doens't increase.\n",
    "                             early_stopping_tolerance=.001,\n",
    "                             early_stopping_min_epochs=2, # No matter what, run these many number of epochs.                             \n",
    "                             num_training_samples=2361) # This is a mandatory parameter, 1464 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'DataSource': {'S3DataSource': {'S3DataType': 'AugmentedManifestFile', 'S3Uri': 's3://st-crayon-dev/data/raw/imgs/tiles_1024/manifests/manifest_file_train_imgs.json', 'S3DataDistributionType': 'FullyReplicated', 'AttributeNames': ['source-ref', 'annotation-ref']}}, 'ContentType': 'application/x-recordio', 'RecordWrapperType': 'RecordIO', 'InputMode': 'Pipe'}\n{'DataSource': {'S3DataSource': {'S3DataType': 'AugmentedManifestFile', 'S3Uri': 's3://st-crayon-dev/data/raw/imgs/tiles_1024/manifests/manifest_file_val_imgs.json', 'S3DataDistributionType': 'FullyReplicated', 'AttributeNames': ['source-ref', 'annotation-ref']}}, 'ContentType': 'application/x-recordio', 'RecordWrapperType': 'RecordIO', 'InputMode': 'Pipe'}\n"
    }
   ],
   "source": [
    "print(f'{train_data.config}\\n{validation_data.config}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:sagemaker:Creating training-job with name: ss-labelbox-train-pipe-1596797102-2020-08-07-10-45-27-126\n2020-08-07 10:45:27 Starting - Starting the training job...\n2020-08-07 10:45:29 Starting - Launching requested ML instances......\n2020-08-07 10:47:03 Starting - Preparing the instances for training............\n2020-08-07 10:49:04 Downloading - Downloading input data\n2020-08-07 10:49:04 Training - Downloading the training image.....\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n\u001b[34mRunning default environment configuration script\u001b[0m\n\u001b[34mRunning custom environment configuration script\u001b[0m\n\u001b[34m[08/07/2020 10:49:59 INFO 139733208078144] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'syncbn': u'False', u'gamma2': u'0.9', u'gamma1': u'0.9', u'early_stopping_min_epochs': u'5', u'epochs': u'10', u'_workers': u'16', u'lr_scheduler_factor': u'0.1', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0001', u'crop_size': u'240', u'use_pretrained_model': u'True', u'_aux_weight': u'0.5', u'_hybrid': u'False', u'_augmentation_type': u'default', u'lr_scheduler': u'poly', u'early_stopping_patience': u'4', u'momentum': u'0.9', u'optimizer': u'sgd', u'early_stopping_tolerance': u'0.0', u'learning_rate': u'0.001', u'backbone': u'resnet-50', u'validation_mini_batch_size': u'16', u'_aux_loss': u'True', u'mini_batch_size': u'16', u'early_stopping': u'False', u'algorithm': u'fcn', u'_logging_frequency': u'20', u'num_training_samples': u'8', u'_kvstore': u'device', u'precision_dtype': u'float32'}\u001b[0m\n\u001b[34m[08/07/2020 10:49:59 INFO 139733208078144] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.0001', u'optimizer': u'rmsprop', u'algorithm': u'deeplab', u'lr_scheduler': u'poly', u'use_pretrained_model': u'True', u'backbone': u'resnet-50', u'base_size': u'1024', u'early_stopping_min_epochs': u'2', u'epochs': u'3', u'early_stopping_tolerance': u'0.001', u'validation_mini_batch_size': u'8', u'num_training_samples': u'2361', u'num_classes': u'8', u'mini_batch_size': u'16', u'early_stopping_patience': u'2', u'early_stopping': u'True', u'crop_size': u'520'}\u001b[0m\n\u001b[34m[08/07/2020 10:49:59 INFO 139733208078144] Final configuration: {u'syncbn': u'False', u'gamma2': u'0.9', u'gamma1': u'0.9', u'base_size': u'1024', u'early_stopping_min_epochs': u'2', u'epochs': u'3', u'_workers': u'16', u'lr_scheduler_factor': u'0.1', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0001', u'crop_size': u'520', u'use_pretrained_model': u'True', u'_aux_weight': u'0.5', u'_hybrid': u'False', u'_augmentation_type': u'default', u'lr_scheduler': u'poly', u'num_classes': u'8', u'early_stopping_patience': u'2', u'momentum': u'0.9', u'optimizer': u'rmsprop', u'early_stopping_tolerance': u'0.001', u'learning_rate': u'0.0001', u'backbone': u'resnet-50', u'validation_mini_batch_size': u'8', u'_aux_loss': u'True', u'mini_batch_size': u'16', u'early_stopping': u'True', u'algorithm': u'deeplab', u'_logging_frequency': u'20', u'num_training_samples': u'2361', u'_kvstore': u'device', u'precision_dtype': u'float32'}\u001b[0m\n\u001b[34mProcess 1 is a worker.\u001b[0m\n\u001b[34m[08/07/2020 10:49:59 INFO 139733208078144] Using default worker.\u001b[0m\n\u001b[34m[08/07/2020 10:49:59 INFO 139733208078144] font search path ['/opt/amazon/lib/python2.7/site-packages/matplotlib/mpl-data/fonts/ttf', '/opt/amazon/lib/python2.7/site-packages/matplotlib/mpl-data/fonts/afm', '/opt/amazon/lib/python2.7/site-packages/matplotlib/mpl-data/fonts/pdfcorefonts']\u001b[0m\n\u001b[34m[08/07/2020 10:49:59 INFO 139733208078144] generated new fontManager\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] Loaded iterator creator application/x-image for content type ('application/x-image', '1.0')\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] Loaded iterator creator application/x-recordio for content type ('application/x-recordio', '1.0')\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] Loaded iterator creator image/png for content type ('image/png', '1.0')\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] Loaded iterator creator application/json for content type ('application/json', '1.0')\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] Loaded iterator creator image/jpeg for content type ('image/jpeg', '1.0')\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] Checkpoint loading and saving are disabled.\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] The channel 'train' is in pipe input mode under /opt/ml/input/data/train.\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] The channel 'train' is in pipe input mode under /opt/ml/input/data/train.\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 WARNING 139733208078144] label maps not provided, using defaults.\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] #label_map train :{'scale': 1}\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] Initializing pipe reader with path: /opt/ml/input/data/train, and epoch: 0\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] The channel 'validation' is in pipe input mode under /opt/ml/input/data/validation.\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] The channel 'validation' is in pipe input mode under /opt/ml/input/data/validation.\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 WARNING 139733208078144] label maps not provided, using defaults.\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] #label_map validation :{'scale': 1}\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] Initializing pipe reader with path: /opt/ml/input/data/validation, and epoch: 0\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] nvidia-smi took: 0.125552177429 secs to identify 8 gpus\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] Number of GPUs being used: 8\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] Number of GPUs being used: 8\u001b[0m\n\u001b[34m[08/07/2020 10:50:00 INFO 139733208078144] Number of GPUs being used: 8\u001b[0m\n\n2020-08-07 10:49:55 Training - Training image download completed. Training in progress.\u001b[34m[10:50:03] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x.1856.0/AL2012/generic-flavor/src/src/storage/storage.cc:108: Using GPUPooledRoundedStorageManager.\u001b[0m\n\u001b[34m[10:50:03] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x.1856.0/AL2012/generic-flavor/src/src/storage/storage.cc:108: Using GPUPooledRoundedStorageManager.\u001b[0m\n\u001b[34m[10:50:04] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x.1856.0/AL2012/generic-flavor/src/src/storage/storage.cc:108: Using GPUPooledRoundedStorageManager.\u001b[0m\n\u001b[34m[10:50:05] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x.1856.0/AL2012/generic-flavor/src/src/storage/storage.cc:108: Using GPUPooledRoundedStorageManager.\u001b[0m\n\u001b[34m[10:50:05] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x.1856.0/AL2012/generic-flavor/src/src/storage/storage.cc:108: Using GPUPooledRoundedStorageManager.\u001b[0m\n\u001b[34m[10:50:06] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x.1856.0/AL2012/generic-flavor/src/src/storage/storage.cc:108: Using GPUPooledRoundedStorageManager.\u001b[0m\n\u001b[34m[10:50:07] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x.1856.0/AL2012/generic-flavor/src/src/storage/storage.cc:108: Using GPUPooledRoundedStorageManager.\u001b[0m\n\u001b[34m[10:50:07] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x.1856.0/AL2012/generic-flavor/src/src/storage/storage.cc:108: Using GPUPooledRoundedStorageManager.\u001b[0m\n\u001b[34m[08/07/2020 10:50:18 INFO 139733208078144] LRScheduler setup: iters per epoch: 147, num_epochs 3\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1596797418.656258, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\"}, \"StartTime\": 1596797418.65587}\n\u001b[0m\n\u001b[34m[08/07/2020 10:51:42 INFO 139733208078144] #progress_notice. epoch: 0, iterations: 20 speed: 7.77425885534 samples/sec learning_rate: 0.000096\u001b[0m\n\u001b[34m[08/07/2020 10:53:00 INFO 139733208078144] #progress_notice. epoch: 0, iterations: 40 speed: 7.74922344792 samples/sec learning_rate: 0.000092\u001b[0m\n\u001b[34m[08/07/2020 10:54:16 INFO 139733208078144] #progress_notice. epoch: 0, iterations: 60 speed: 7.68927603399 samples/sec learning_rate: 0.000088\u001b[0m\n\u001b[34m[08/07/2020 10:55:34 INFO 139733208078144] #progress_notice. epoch: 0, iterations: 80 speed: 7.75133759327 samples/sec learning_rate: 0.000084\u001b[0m\n\u001b[34m[08/07/2020 10:56:52 INFO 139733208078144] #progress_notice. epoch: 0, iterations: 100 speed: 7.65692034842 samples/sec learning_rate: 0.000079\u001b[0m\n\u001b[34m[08/07/2020 10:58:09 INFO 139733208078144] #progress_notice. epoch: 0, iterations: 120 speed: 7.6861769367 samples/sec learning_rate: 0.000075\u001b[0m\n\u001b[34m[08/07/2020 10:59:26 INFO 139733208078144] #progress_notice. epoch: 0, iterations: 140 speed: 7.66584615251 samples/sec learning_rate: 0.000071\u001b[0m\n\u001b[34m[08/07/2020 10:59:54 INFO 139733208078144] #quality_metric. host: algo-1, epoch: 0, train loss: 1.7798295503064079 .\u001b[0m\n\u001b[34m[08/07/2020 10:59:54 INFO 139733208078144] #throughput_metric. host: algo-1, epoch: 0, train throughput: 7.53919186404 samples/sec.\u001b[0m\n\u001b[34m[08/07/2020 11:00:13 INFO 139733208078144] #progress_notice. epoch: 0, iterations: 20 speed: 18.2782336897 samples/sec\u001b[0m\n\u001b[34m[08/07/2020 11:00:25 INFO 139733208078144] #quality_metric. host: algo-1, epoch: 0, validation pixel_accuracy: 0.6454871995894306 .\u001b[0m\n\u001b[34m[08/07/2020 11:00:25 INFO 139733208078144] #quality_metric. host: algo-1, epoch: 0, validation mIOU: 0.24552499552500487 .\u001b[0m\n\u001b[34m[08/07/2020 11:00:25 INFO 139733208078144] #throughput_metric. host: algo-1, epoch: 0, validation throughput: 18.2514370428 samples/sec.\u001b[0m\n\u001b[34m[08/07/2020 11:00:25 INFO 139733208078144] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n\u001b[34m[08/07/2020 11:00:25 INFO 139733208078144] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n\u001b[34m[08/07/2020 11:00:25 INFO 139733208078144] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1596798025.815686, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 0}, \"StartTime\": 1596797418.656704}\n\u001b[0m\n\u001b[34m[08/07/2020 11:00:25 INFO 139733208078144] Initializing pipe reader with path: /opt/ml/input/data/train, and epoch: 1\u001b[0m\n\u001b[34m[08/07/2020 11:00:25 INFO 139733208078144] Initializing pipe reader with path: /opt/ml/input/data/validation, and epoch: 1\u001b[0m\n\u001b[34m[08/07/2020 11:01:42 INFO 139733208078144] #progress_notice. epoch: 1, iterations: 20 speed: 7.7806593719 samples/sec learning_rate: 0.000065\u001b[0m\n\u001b[34m[08/07/2020 11:02:59 INFO 139733208078144] #progress_notice. epoch: 1, iterations: 40 speed: 7.63008937054 samples/sec learning_rate: 0.000061\u001b[0m\n\u001b[34m[08/07/2020 11:04:16 INFO 139733208078144] #progress_notice. epoch: 1, iterations: 60 speed: 7.72446434865 samples/sec learning_rate: 0.000057\u001b[0m\n\u001b[34m[08/07/2020 11:05:34 INFO 139733208078144] #progress_notice. epoch: 1, iterations: 80 speed: 7.67236316787 samples/sec learning_rate: 0.000052\u001b[0m\n\u001b[34m[08/07/2020 11:06:50 INFO 139733208078144] #progress_notice. epoch: 1, iterations: 100 speed: 7.68966018254 samples/sec learning_rate: 0.000048\u001b[0m\n\u001b[34m[08/07/2020 11:08:08 INFO 139733208078144] #progress_notice. epoch: 1, iterations: 120 speed: 7.68978618457 samples/sec learning_rate: 0.000043\u001b[0m\n\u001b[34m[08/07/2020 11:09:27 INFO 139733208078144] #progress_notice. epoch: 1, iterations: 140 speed: 7.6649749602 samples/sec learning_rate: 0.000039\u001b[0m\n\u001b[34m[08/07/2020 11:09:57 INFO 139733208078144] #quality_metric. host: algo-1, epoch: 1, train loss: 1.5616107648112685 .\u001b[0m\n\u001b[34m[08/07/2020 11:09:57 INFO 139733208078144] #throughput_metric. host: algo-1, epoch: 1, train throughput: 7.64732577281 samples/sec.\u001b[0m\n\u001b[34m[08/07/2020 11:10:15 INFO 139733208078144] #progress_notice. epoch: 1, iterations: 20 speed: 18.5676478644 samples/sec\u001b[0m\n\u001b[34m[08/07/2020 11:10:28 INFO 139733208078144] #quality_metric. host: algo-1, epoch: 1, validation pixel_accuracy: 0.7185933107080629 .\u001b[0m\n\u001b[34m[08/07/2020 11:10:28 INFO 139733208078144] #quality_metric. host: algo-1, epoch: 1, validation mIOU: 0.3339515634437994 .\u001b[0m\n\u001b[34m[08/07/2020 11:10:28 INFO 139733208078144] #throughput_metric. host: algo-1, epoch: 1, validation throughput: 18.3825474346 samples/sec.\u001b[0m\n\u001b[34m[08/07/2020 11:10:28 INFO 139733208078144] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n\u001b[34m[08/07/2020 11:10:28 INFO 139733208078144] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1596798628.66983, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 1}, \"StartTime\": 1596798025.815838}\n\u001b[0m\n\u001b[34m[08/07/2020 11:10:28 INFO 139733208078144] Initializing pipe reader with path: /opt/ml/input/data/train, and epoch: 2\u001b[0m\n\u001b[34m[08/07/2020 11:10:28 INFO 139733208078144] Initializing pipe reader with path: /opt/ml/input/data/validation, and epoch: 2\u001b[0m\n\u001b[34m[08/07/2020 11:11:45 INFO 139733208078144] #progress_notice. epoch: 2, iterations: 20 speed: 7.53029128055 samples/sec learning_rate: 0.000032\u001b[0m\n\u001b[34m[08/07/2020 11:13:03 INFO 139733208078144] #progress_notice. epoch: 2, iterations: 40 speed: 7.65929561592 samples/sec learning_rate: 0.000028\u001b[0m\n\u001b[34m[08/07/2020 11:14:21 INFO 139733208078144] #progress_notice. epoch: 2, iterations: 60 speed: 7.68761829178 samples/sec learning_rate: 0.000023\u001b[0m\n\u001b[34m[08/07/2020 11:15:39 INFO 139733208078144] #progress_notice. epoch: 2, iterations: 80 speed: 7.3865642385 samples/sec learning_rate: 0.000018\u001b[0m\n\u001b[34m[08/07/2020 11:16:57 INFO 139733208078144] #progress_notice. epoch: 2, iterations: 100 speed: 7.61470030808 samples/sec learning_rate: 0.000013\u001b[0m\n\u001b[34m[08/07/2020 11:18:14 INFO 139733208078144] #progress_notice. epoch: 2, iterations: 120 speed: 7.7149075552 samples/sec learning_rate: 0.000008\u001b[0m\n\u001b[34m[08/07/2020 11:19:32 INFO 139733208078144] #progress_notice. epoch: 2, iterations: 140 speed: 7.51425913872 samples/sec learning_rate: 0.000002\u001b[0m\n\u001b[34m[08/07/2020 11:20:00 INFO 139733208078144] #quality_metric. host: algo-1, epoch: 2, train loss: 1.427228977054763 .\u001b[0m\n\u001b[34m[08/07/2020 11:20:00 INFO 139733208078144] #throughput_metric. host: algo-1, epoch: 2, train throughput: 7.6574524152 samples/sec.\u001b[0m\n\u001b[34m[08/07/2020 11:20:18 INFO 139733208078144] #progress_notice. epoch: 2, iterations: 20 speed: 18.6588904682 samples/sec\u001b[0m\n\n2020-08-07 11:20:38 Uploading - Uploading generated training model\u001b[34m[08/07/2020 11:20:30 INFO 139733208078144] #quality_metric. host: algo-1, epoch: 2, validation pixel_accuracy: 0.7614942141998942 .\u001b[0m\n\u001b[34m[08/07/2020 11:20:30 INFO 139733208078144] #quality_metric. host: algo-1, epoch: 2, validation mIOU: 0.35139564927715244 .\u001b[0m\n\u001b[34m[08/07/2020 11:20:30 INFO 139733208078144] #throughput_metric. host: algo-1, epoch: 2, validation throughput: 18.2767568879 samples/sec.\u001b[0m\n\u001b[34m[08/07/2020 11:20:30 INFO 139733208078144] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n\u001b[34m[08/07/2020 11:20:31 INFO 139733208078144] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1596799231.191069, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 2}, \"StartTime\": 1596798628.66998}\n\u001b[0m\n\u001b[34m[08/07/2020 11:20:31 WARNING 139733208078144] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n\u001b[34m[08/07/2020 11:20:31 INFO 139733208078144] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n\u001b[34m[08/07/2020 11:20:31 INFO 139733208078144] Test data is not provided.\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}, \"totaltime\": {\"count\": 1, \"max\": 1832381.2410831451, \"sum\": 1832381.2410831451, \"min\": 1832381.2410831451}, \"setuptime\": {\"count\": 1, \"max\": 18.501996994018555, \"sum\": 18.501996994018555, \"min\": 18.501996994018555}}, \"EndTime\": 1596799231.522011, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\"}, \"StartTime\": 1596797400.434431}\n\u001b[0m\n\n2020-08-07 11:21:24 Completed - Training job completed\nTraining seconds: 1947\nBillable seconds: 584\nManaged Spot Training savings: 70.0%\n"
    }
   ],
   "source": [
    "ss_model.fit(inputs=data_channels,\n",
    "             logs=True,\n",
    "             experiment_config={\n",
    "                \"TrialName\": ss_trial.trial_name,\n",
    "                \"TrialComponentDisplayName\": \"Training\",\n",
    "                },\n",
    "             wait=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model : Go to notebook site_data_model_deployment.ipynb\n",
    "code below is for expermentation and need to be cleaned before final delivery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}